{"metadata":{"colab":{"provenance":[],"gpuType":"T4","collapsed_sections":["mhWHvUiaBptZ"]},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","kaggle":{"accelerator":"none","dataSources":[{"sourceId":9187498,"sourceType":"datasetVersion","datasetId":5553802}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Clone Repo","metadata":{"_uuid":"1d603412-f0dc-43ca-9cc4-052d85102650","_cell_guid":"60b2b197-19ec-4c25-b87a-b503832691fa","id":"b5TtXswcBvWE","trusted":true}},{"cell_type":"code","source":"!git clone https://github.com/Janhvi0103/SICKLE.git","metadata":{"_uuid":"79ddba24-8d79-4a23-bda0-f478a4d8eb16","_cell_guid":"bf93c4f9-6ec5-4c5c-aaa1-199f706a7bc6","collapsed":false,"id":"cXqCUIR0DwwR","outputId":"4c200996-66ee-4d37-a2aa-6d8b8ff0b17b","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-08-23T14:41:45.968372Z","iopub.execute_input":"2024-08-23T14:41:45.968713Z","iopub.status.idle":"2024-08-23T14:41:46.933977Z","shell.execute_reply.started":"2024-08-23T14:41:45.968684Z","shell.execute_reply":"2024-08-23T14:41:46.933104Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Cloning into 'SICKLE'...\nremote: Enumerating objects: 162, done.\u001b[K\nremote: Counting objects: 100% (111/111), done.\u001b[K\nremote: Compressing objects: 100% (81/81), done.\u001b[K\nremote: Total 162 (delta 39), reused 92 (delta 30), pack-reused 51 (from 1)\u001b[K\nReceiving objects: 100% (162/162), 2.46 MiB | 34.51 MiB/s, done.\nResolving deltas: 100% (66/66), done.\n","output_type":"stream"}]},{"cell_type":"code","source":"%cd SICKLE","metadata":{"_uuid":"ad78d637-9527-4ba1-9fe0-49964947b2ae","_cell_guid":"717f839f-638c-4bb1-934e-d0ea6a12b311","collapsed":false,"id":"9Nw1QtTJu2cZ","outputId":"c9fd9726-ff06-409b-8db5-2e3a783965d8","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-08-23T14:41:46.935673Z","iopub.execute_input":"2024-08-23T14:41:46.936002Z","iopub.status.idle":"2024-08-23T14:41:46.941191Z","shell.execute_reply.started":"2024-08-23T14:41:46.935968Z","shell.execute_reply":"2024-08-23T14:41:46.940585Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"/kaggle/working/SICKLE\n","output_type":"stream"}]},{"cell_type":"code","source":"with open('train.py', 'r') as file:\n    content = file.readlines()\nprint(content)    ","metadata":{"execution":{"iopub.status.busy":"2024-08-23T14:41:46.942035Z","iopub.execute_input":"2024-08-23T14:41:46.942262Z","iopub.status.idle":"2024-08-23T14:41:46.953049Z","shell.execute_reply.started":"2024-08-23T14:41:46.942238Z","shell.execute_reply":"2024-08-23T14:41:46.952468Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"['#  conda install pytorch torchvision torchaudio pytorch-cuda=11.6 \\\\\\n', '    \\n', '# -c pytorch -c nvidia -c conda-forge\\n', '\"\"\"\\n', 'Main script for semantic experiments\\n', 'Built upon Vivien Sainte Fare Garnot (github/VSainteuf)\\n', 'License: MIT\\n', '\"\"\"\\n', '\\n', 'import argparse\\n', 'import json\\n', 'import os\\n', 'import copy\\n', 'import wandb\\n', 'import pprint\\n', '\\n', 'import time\\n', 'import random\\n', 'import pandas as pd\\n', 'import numpy as np\\n', 'from tqdm import tqdm\\n', '\\n', '# Custom import\\n', 'from utils.dataset import SICKLE_Dataset\\n', 'from utils import utae_utils, model_utils\\n', 'from utils.weight_init import weight_init\\n', 'from utils.metric import get_metrics, RMSELoss\\n', '# torch\\n', 'import torch\\n', 'import torch.nn as nn\\n', 'import torch.utils.data as data\\n', 'from torch.optim.lr_scheduler import CosineAnnealingLR\\n', 'import torchnet as tnt\\n', '\\n', 'parser = argparse.ArgumentParser()\\n', '# Model parameters\\n', 'parser.add_argument(\\n', '    \"--model\",\\n', '    default=\"utae\",\\n', '    type=str,\\n', '    help=\"Type of architecture to use. Can be one of: (utae/unet3d/fpn/convlstm/convgru/uconvlstm/buconvlstm)\",\\n', ')\\n', '## U-TAE Hyperparameters\\n', 'parser.add_argument(\"--encoder_widths\", default=\"[64,128]\", type=str)\\n', 'parser.add_argument(\"--decoder_widths\", default=\"[32,128]\", type=str)\\n', 'parser.add_argument(\"--out_conv\", default=\"[32, 16]\")\\n', 'parser.add_argument(\"--str_conv_k\", default=4, type=int)\\n', 'parser.add_argument(\"--str_conv_s\", default=2, type=int)\\n', 'parser.add_argument(\"--str_conv_p\", default=1, type=int)\\n', 'parser.add_argument(\"--agg_mode\", default=\"att_group\", type=str)\\n', 'parser.add_argument(\"--encoder_norm\", default=\"group\", type=str)\\n', 'parser.add_argument(\"--n_head\", default=16, type=int)\\n', 'parser.add_argument(\"--d_model\", default=256, type=int)\\n', 'parser.add_argument(\"--d_k\", default=4, type=int)\\n', '\\n', '# Set-up parameters\\n', 'parser.add_argument(\\n', '    \"--device\",\\n', '    default= \"cuda\" if torch.cuda.is_available() else \"cpu\",\\n', '    type=str,\\n', '    help=\"Name of device to use for tensor computations (cuda/cpu)\",\\n', ')\\n', 'parser.add_argument(\\n', '    \"--num_workers\", default=8, type=int, help=\"Number of data loading workers\"\\n', ')\\n', 'parser.add_argument(\"--seed\", default=0, type=int, help=\"Random seed\")\\n', '# Training parameters\\n', 'parser.add_argument(\"--epochs\", default=100, type=int, help=\"Number of epochs per fold\")\\n', 'parser.add_argument(\"--batch_size\", default=32, type=int, help=\"Batch size\")\\n', 'parser.add_argument(\"--lr\", default=1e-1, type=float, help=\"Learning rate\")\\n', '# parser.add_argument(\"--wd\", default=1e-2, type=float, help=\"weight decay\")\\n', 'parser.add_argument(\"--num_classes\", default=2, type=int)\\n', 'parser.add_argument(\"--ignore_index\", default=-999, type=int)\\n', 'parser.add_argument(\"--pad_value\", default=0, type=float)\\n', 'parser.add_argument(\"--padding_mode\", default=\"reflect\", type=str)\\n', 'parser.add_argument(\"--resume\", default=\"\", type=str, help=\"enter run path to resume\")\\n', 'parser.add_argument(\"--run_id\", default=\"\", type=str, help=\"enter run id to resume\")\\n', 'parser.add_argument(\"--wandb\", action=\\'store_true\\', help=\"debug?\")\\n', 'parser.add_argument(\\'--satellites\\', type=str, default=\"[S2]\")\\n', 'parser.add_argument(\\'--run_name\\', type=str, default=\"trial\")\\n', 'parser.add_argument(\\'--exp_name\\', type=str, default=\"utae\")\\n', 'parser.add_argument(\\'--task\\', type=str, default=\"crop_type\",\\n', '                    help=\"Available Tasks are crop_type, sowing_date, transplanting_date, harvesting_date, crop_yield\")\\n', 'parser.add_argument(\\'--actual_season\\', action=\\'store_true\\', help=\"whether to consider actual season or not.\")\\n', 'parser.add_argument(\\'--data_dir\\', type=str, default=\"../sickle_dev/data\")\\n', \"parser.add_argument('--use_augmentation', type=bool, default=True)\\n\", '\\n', 'list_args = [\"encoder_widths\", \"decoder_widths\", \"out_conv\", \"satellites\"]\\n', 'parser.set_defaults(cache=False)\\n', '\\n', '\\n', 'def recursive_todevice(x, device):\\n', '    if isinstance(x, torch.Tensor):\\n', '        return x.to(device)\\n', '    elif isinstance(x, dict):\\n', '        return {k: recursive_todevice(v, device) for k, v in x.items()}\\n', '    else:\\n', '        return [recursive_todevice(c, device) for c in x]\\n', '\\n', '\\n', 'def prepare_output(CFG):\\n', '    if CFG.wandb:\\n', '        if not os.path.exists(CFG.run_path):\\n', '            os.makedirs(CFG.run_path)\\n', '        elif CFG.resume:\\n', '            pass\\n', '        else:\\n', '            CFG.run_path = CFG.run_path + f\"_{time.time()}\"\\n', '            print(\"Run path already exist changed run path to \", CFG.run_path)\\n', '            os.makedirs(CFG.run_path)\\n', '    else:\\n', '        CFG.run_path += \"_debug\"\\n', '        os.makedirs(CFG.run_path, exist_ok=True)\\n', '\\n', '\\n', 'def checkpoint(log, config):\\n', '    with open(\\n', '            os.path.join(config.run_path, \"trainlog.json\"), \"w\"\\n', '    ) as outfile:\\n', '        json.dump(log, outfile, indent=4)\\n', '\\n', '\\n', 'def set_seed(seed=42):\\n', '    # Set a fixed value for the hash seed\\n', '    os.environ[\"PYTHONHASHSEED\"] = str(seed)\\n', '\\n', '    # For reproducibility\\n', '    np.random.seed(seed)\\n', '    random.seed(seed)\\n', '    torch.manual_seed(seed)\\n', '    torch.cuda.manual_seed(seed)\\n', '    torch.backends.cudnn.deterministic = True\\n', '    torch.backends.cudnn.benchmark = False\\n', '    try:\\n', '        torch.use_deterministic_algorithms(True, warn_only=True)\\n', '    except Exception as e:\\n', '        print(\"Can not use deterministic algorithm. Error: \", e)\\n', '    print(f\"> SEEDING DONE {seed}\")\\n', '\\n', '\\n', 'def log_wandb(loss, metrics, table=None, phase=\"train\"):\\n', '    f1_macro, acc, iou, f1_paddy, f1_non_paddy, \\\\\\n', '    acc_paddy, acc_non_paddy, iou_paddy, iou_non_paddy, (y_pred, y_true) = metrics\\n', '    y_pred, y_true = y_pred.tolist(), y_true.tolist()\\n', '    if CFG.wandb:\\n', '        log = {\\n', '                f\"{phase}_loss\": loss,\\n', '                f\"{phase}_f1_macro\": f1_macro,\\n', '                f\"{phase}_acc\": acc,\\n', '                f\"{phase}_iou\": iou,\\n', '                f\"{phase}_f1_paddy\": f1_paddy,\\n', '                f\"{phase}_f1_non_paddy\": f1_non_paddy,\\n', '                f\"{phase}_acc_paddy\": acc_paddy,\\n', '                f\"{phase}_acc_non_paddy\": acc_non_paddy,\\n', '                f\"{phase}_iou_paddy\": iou_paddy,\\n', '                f\"{phase}_iou_non_paddy\": iou_non_paddy,\\n', '            }\\n', '        if table is not None:\\n', '            log[table[\"key\"]] = table[\"value\"]\\n', '        wandb.log(log)\\n', '        if phase == \"test\":\\n', '            wandb.log({f\"{phase}_conf_mat\": wandb.plot.confusion_matrix(y_true=y_true, preds=y_pred, probs=None,\\n', '                                                                        class_names=[\"Paddy\", \"Non Paddy\"])})\\n', '\\n', '\\n', 'def iterate(\\n', '        model, data_loader, criterion, optimizer=None, scheduler=None, mode=\"train\", epoch=1, task=\"crop_type\",\\n', '        device=None, log=False, CFG=None,\\n', '):\\n', '    loss_meter = tnt.meter.AverageValueMeter()\\n', '    predictions = None\\n', '    targets = None\\n', '    pid_masks =None\\n', '    if log:\\n', '        columns = [\"image_l8\", \"image_s2\", \"image_s1\", \"gt_mask\", \"pred_filtered\", \"pred_whole\"]\\n', '        wandb_table = wandb.Table(columns=columns)\\n', '\\n', '    t_start = time.time()\\n', '    pbar = tqdm(enumerate(data_loader), total=len(data_loader), desc=mode)\\n', '    for i, batch in pbar:\\n', '        if device is not None:\\n', '            batch = recursive_todevice(batch, device)\\n', '        data, masks = batch\\n', '        plot_mask = masks[\"plot_mask\"]\\n', '        masks = masks[task]\\n', '        if task == \"crop_type\":\\n', '            masks = masks.long()\\n', '        else:\\n', '            masks = masks.float()\\n', '        if mode != \"train\":\\n', '            with torch.no_grad():\\n', '                y_pred = model(data)\\n', '        else:\\n', '            optimizer.zero_grad()\\n', '            y_pred = model(data)\\n', '        if task==\"crop_yield\": \\n', '            loss = criterion(y_pred, masks, plot_mask)\\n', '        else:\\n', '            loss = criterion(y_pred, masks)\\n', '            \\n', '        if mode == \"train\":\\n', '            loss.backward()\\n', '            optimizer.step()\\n', '\\n', '        # Compute Metric\\n', '        if task == \"crop_type\":\\n', '            y_pred = nn.Softmax(dim=1)(y_pred)\\n', '\\n', '        if predictions is None:\\n', '            predictions = y_pred\\n', '            targets = masks\\n', '            pid_masks = plot_mask\\n', '        else:\\n', '            predictions = torch.cat([predictions, y_pred], dim=0)\\n', '            targets = torch.cat([targets, masks], dim=0)\\n', '            pid_masks = torch.cat([pid_masks, plot_mask], dim=0)\\n', '            \\n', '\\n', '        if log:\\n', '            if len(data.keys()) == 3:\\n', '                (l8_images, l8_dates) = data[\"L8\"]\\n', '                (s2_images, s2_dates) = data[\"S2\"]\\n', '                (s1_images, s1_dates) = data[\"S1\"]\\n', '            else:\\n', '                (l8_images, l8_dates) = data[CFG.primary_sat]\\n', '                (s2_images, s2_dates) = data[CFG.primary_sat]\\n', '                (s1_images, s1_dates) = data[CFG.primary_sat]\\n', '            if task == \"crop_type\":\\n', '                # y_pred = torch.argmax(nn.Softmax(dim=1)(y_pred), dim=1)\\n', '                y_pred = nn.Softmax(dim=1)(y_pred)[:, 0, :, :]\\n', '                # log image of primary satellite\\n', '                l8_images, s2_images, s1_images, l8_dates, s2_dates, s1_dates, y_pred, masks = \\\\\\n', '                    l8_images.cpu().numpy(), s2_images.cpu().numpy(), s1_images.cpu().numpy(), \\\\\\n', '                    l8_dates.cpu().numpy(), s2_dates.cpu().numpy(), s1_dates.cpu().numpy(), \\\\\\n', '                    y_pred.cpu().numpy(), masks.cpu().numpy()\\n', '            else:\\n', '                # log image of primary satellite\\n', '                y_pred = y_pred[:, 0, :, :]\\n', '                l8_images, s2_images, s1_images, l8_dates, s2_dates, s1_dates, y_pred, masks = \\\\\\n', '                    l8_images.cpu().numpy(), s2_images.cpu().numpy(), s1_images.cpu().numpy(), \\\\\\n', '                    l8_dates.cpu().numpy(), s2_dates.cpu().numpy(), s1_dates.cpu().numpy(), \\\\\\n', '                    y_pred.cpu().numpy(), masks.cpu().numpy()\\n', '            log_test_predictions(l8_images, s2_images, s1_images, l8_dates, s2_dates, s1_dates, masks, y_pred, wandb_table, CFG=CFG)\\n', '\\n', '        loss_meter.add(loss.item())\\n', '\\n', '        # Just for Monitoring\\n', '        mem = torch.cuda.memory_reserved() / 1e9 if torch.cuda.is_available() else 0\\n', '        pbar.set_postfix(\\n', '            Loss=f\"{loss.item():0.4f}\",\\n', '            gpu_mem=f\"{mem:0.2f} GB\",\\n', '        )\\n', '    # take scheduler step\\n', '    if scheduler is not None and epoch < 3 * CFG.epochs // 4:\\n', '        scheduler.step()\\n', '\\n', '    t_end = time.time()\\n', '    total_time = t_end - t_start\\n', '    print(\"Epoch time : {:.1f}s\".format(total_time))\\n', '    metrics = get_metrics(predictions, targets, pid_masks, ignore_index=CFG.ignore_index, task=task)\\n', '    if log:\\n', '        return loss_meter.value()[0], metrics, wandb_table\\n', '    return loss_meter.value()[0], metrics\\n', '\\n', '\\n', 'n_log = 10  # no of samples to log\\n', '\\n', 'def generate_heatmap(mask):\\n', '    import matplotlib.pyplot as plt\\n', '    import seaborn as sns\\n', '    fig = plt.figure()\\n', '    hm = sns.heatmap(data=mask, vmin=-1, vmax=1 if np.max(mask) <= 1 else np.max(mask),\\n', \"                     cmap='RdYlGn')\\n\", \"    plt.axis('off')\\n\", '    fig.canvas.draw()\\n', '    mask = np.frombuffer(fig.canvas.tostring_rgb(), dtype=np.uint8)\\n', '    mask = mask.reshape(fig.canvas.get_width_height()[::-1] + (3,))\\n', '    return mask\\n', '\\n', '\\n', 'def log_test_predictions(l8_images, s2_images, s1_images, l8_dates, s2_dates, s1_dates, gt_masks, pred_masks, test_table, CFG = None, task=\"crop_type\", ):\\n', '    _id = 0\\n', '    # print(gt_masks.shape,pred_masks.shape)\\n', '    # pred_masks[pred_masks == 1] = 128\\n', '    gt_masks[gt_masks == -999] = -1\\n', '\\n', '\\n', '    # print(np.unique(pred_masks))\\n', '    for l8_sample, s2_sample, s1_sample, l8_sample_dates, s2_sample_dates, s1_sample_dates, gt_mask, pred_mask in \\\\\\n', '            zip(l8_images, s2_images, s1_images, l8_dates, s2_dates, s1_dates, gt_masks, pred_masks):\\n', '        # get last available image\\n', '        l8_image = l8_sample[len(l8_sample_dates[l8_sample_dates != 0]) - 1]\\n', '        # reshape and normalize image\\n', '        l8_image = l8_image[\\n', '            CFG.satellites[\"L8\" if len(CFG.satellites) == 3 else CFG.primary_sat][\"rgb_bands\"]].transpose(1, 2, 0)\\n', '        l8_image = ((l8_image - np.min(l8_image)) / (np.max(l8_image) - np.min(l8_image)))\\n', '\\n', '        s2_image = s2_sample[len(s2_sample_dates[s2_sample_dates != 0]) - 1]\\n', '        # reshape and normalize image\\n', '        s2_image = s2_image[\\n', '            CFG.satellites[\"S2\" if len(CFG.satellites) == 3 else CFG.primary_sat][\"rgb_bands\"]].transpose(1, 2, 0)\\n', '        s2_image = ((s2_image - np.min(s2_image)) / (np.max(s2_image) - np.min(s2_image)))\\n', '\\n', '        s1_image = s1_sample[len(s1_sample_dates[s1_sample_dates != 0]) - 1]\\n', '        # reshape and normalize image\\n', '        s1_image = s1_image[\\n', '            CFG.satellites[\"S1\" if len(CFG.satellites) == 3 else CFG.primary_sat][\"rgb_bands\"]].transpose(1, 2, 0)\\n', '        s1_image = ((s1_image - np.min(s1_image)) / (np.max(s1_image) - np.min(s1_image)))\\n', '\\n', '        # log whole prediction mask\\n', '        pred_mask_whole = generate_heatmap(copy.deepcopy(pred_mask))\\n', '        pred_mask[gt_mask == -1] = -1\\n', '        pred_mask = generate_heatmap(copy.deepcopy(pred_mask))\\n', '        if task == \"crop_type\":\\n', '            gt_mask[gt_mask == 0] = 2\\n', '            gt_mask[gt_mask == 1] = 0\\n', '            gt_mask[gt_mask == 2] = 1\\n', '        gt_mask = generate_heatmap(copy.deepcopy(gt_mask))\\n', '\\n', '        test_table.add_data(wandb.Image(l8_image), wandb.Image(s2_image), wandb.Image(s1_image), wandb.Image(gt_mask),\\n', '                            wandb.Image(pred_mask), wandb.Image(pred_mask_whole))\\n', '        _id += 1\\n', '        if _id == n_log:\\n', '            break\\n', '\\n', '\\n', 'def main(CFG):\\n', '    prepare_output(CFG)\\n', '    device = torch.device(CFG.device)\\n', '\\n', '    # Dataset definition\\n', '    data_dir = CFG.data_dir\\n', '    df = pd.read_csv(os.path.join(data_dir,\"sickle_dataset_tabular.csv\"))\\n', '    # if \"S2\" in CFG.satellites.keys():\\n', '    #     df = df[df[f\"S2_available\"] == True].reset_index(drop=True)\\n', '    # else:\\n', '    #     df = df[df[f\"{CFG.primary_sat}_available\"] == True].reset_index(drop=True)\\n', '    if CFG.task != \"crop_type\":\\n', '        df = df[df.YIELD > 0].reset_index(drop=True)\\n', '\\n', '    train_df = df[df.SPLIT == \"train\"].reset_index(drop=True)\\n', '    val_df = df[df.SPLIT == \"val\"].reset_index(drop=True)\\n', '    test_df = df[df.SPLIT == \"test\"].reset_index(drop=True)\\n', '\\n', '    dt_args = dict(\\n', '        data_dir=data_dir,\\n', '        satellites=CFG.satellites,\\n', '        ignore_index=CFG.ignore_index,\\n', '        transform=CFG.use_augmentation,\\n', '        actual_season=CFG.actual_season\\n', '    )\\n', '\\n', '    dt_train = SICKLE_Dataset(df=train_df, phase=\"train\", **dt_args)\\n', '    dt_args = dict(\\n', '        data_dir=data_dir,\\n', '        satellites=CFG.satellites,\\n', '        ignore_index=CFG.ignore_index,\\n', '        actual_season=CFG.actual_season\\n', '    )\\n', '    dt_val = SICKLE_Dataset(df=val_df, **dt_args, )\\n', '    dt_test = SICKLE_Dataset(df=test_df, **dt_args)\\n', '\\n', '    collate_fn = lambda x: utae_utils.pad_collate(x, pad_value=CFG.pad_value)\\n', '    train_loader = data.DataLoader(\\n', '        dt_train,\\n', '        batch_size=CFG.batch_size,\\n', '        shuffle=True,\\n', '        collate_fn=collate_fn,\\n', '        num_workers=CFG.num_workers,\\n', '    )\\n', '    val_loader = data.DataLoader(\\n', '        dt_val,\\n', '        batch_size=CFG.batch_size,\\n', '        shuffle=False,\\n', '        collate_fn=collate_fn,\\n', '        num_workers=CFG.num_workers,\\n', '    )\\n', '    test_loader = data.DataLoader(\\n', '        dt_test,\\n', '        batch_size=CFG.batch_size,\\n', '        shuffle=False,\\n', '        collate_fn=collate_fn,\\n', '        num_workers=CFG.num_workers,\\n', '    )\\n', '    batch_data, masks = next(iter(train_loader))\\n', '    for sat in CFG.satellites.keys():\\n', '        (samples, dates) = batch_data[sat]\\n', '        print(f\"-----------{sat}------------\")\\n', '        print(\"Samples Shape\", samples.shape, \"Masks Shape\", masks[\"crop_type\"].shape)\\n', '        print(\"dates\", dates[0])\\n', '        print(\"Samples\", torch.unique(samples[0]))\\n', '        print(\"Masks\", torch.unique(masks[CFG.task]))\\n', '\\n', '    print(\\n', '        \"Train {}, Val {}, Test {}\".format(len(dt_train), len(dt_val), len(dt_test))\\n', '    )\\n', '\\n', '    # Model definition\\n', '    # if len(CFG.satellites)==1:\\n', '    #     print(\"Using Build model\")\\n', '    #     model = model_utils.Build_model(CFG)\\n', '    # else:\\n', '    #     print(\"Using Fusion model\")\\n', '    #     model = model_utils.Fusion_model(CFG)\\n', '    model = model_utils.Fusion_model(CFG)\\n', '    model.apply(weight_init)\\n', '    model = model.to(device)\\n', '    CFG.N_params = utae_utils.get_ntrainparams(model)\\n', '    print(\"TOTAL TRAINABLE PARAMETERS :\", CFG.N_params)\\n', '    with open(os.path.join(CFG.run_path, \"conf.json\"), \"w\") as file:\\n', '        file.write(json.dumps(vars(CFG), indent=4))\\n', '\\n', '    # Optimizer, Loss and Scheduler\\n', '    optimizer = torch.optim.Adam(model.parameters(), lr=CFG.lr)\\n', '    if CFG.task == \"crop_type\":\\n', '        criterion = nn.CrossEntropyLoss(ignore_index=CFG.ignore_index,\\n', '                                        weight=torch.tensor([0.62013, 0.37987])).to(device=CFG.device, dtype=torch.float32)\\n', '    else:\\n', '        criterion = RMSELoss(ignore_index=CFG.ignore_index)\\n', '    scheduler = CosineAnnealingLR(optimizer, T_max=3 * CFG.epochs // 4, eta_min=1e-4)\\n', '\\n', '    # Training loop\\n', '    trainlog = {}\\n', '    best_metric = 0 if CFG.task == \"crop_type\" else torch.inf\\n', '    for epoch in range(1, CFG.epochs + 1):\\n', '        print(\"EPOCH {}/{}\".format(epoch, CFG.epochs))\\n', '        model.train()\\n', '        train_loss, train_metrics = iterate(\\n', '            model,\\n', '            data_loader=train_loader,\\n', '            criterion=criterion,\\n', '            optimizer=optimizer,\\n', '            scheduler=scheduler,\\n', '            mode=\"train\",\\n', '            device=device,\\n', '            epoch=epoch,\\n', '            task=CFG.task,\\n', '            CFG=CFG,\\n', '        )\\n', '\\n', '        print(\"Validation . . . \")\\n', '        model.eval()\\n', '        val_loss, val_metrics = iterate(\\n', '            model,\\n', '            data_loader=val_loader,\\n', '            criterion=criterion,\\n', '            optimizer=optimizer,\\n', '            mode=\"val\",\\n', '            device=device,\\n', '            task=CFG.task,\\n', '            CFG=CFG,\\n', '        )\\n', \"        lr = optimizer.param_groups[0]['lr']\\n\", '        if CFG.task == \"crop_type\":\\n', '            # train metrics\\n', '            train_f1_macro, train_acc, train_iou, train_f1_paddy, train_f1_non_paddy, \\\\\\n', '            train_acc_paddy, train_acc_non_paddy, train_iou_paddy, train_iou_non_paddy, _ = train_metrics\\n', '            # val metric\\n', '            val_f1_macro, val_acc, val_iou, val_f1_paddy, val_f1_non_paddy, \\\\\\n', '            val_acc_paddy, val_acc_non_paddy, val_iou_paddy, val_iou_non_paddy, _ = val_metrics\\n', '            deciding_metric = val_f1_macro\\n', '            # log and print metrics\\n', '            print(\\n', '                f\"F1: {val_f1_macro:0.4f} | Paddy F1: {val_f1_paddy:0.4f} | Non-Paddy F1: {val_f1_non_paddy:0.4f} \\\\nAcc:{val_acc:0.4f} | Paddy Acc: {val_acc_paddy:0.4f} | Non-Paddy Acc: {val_acc_non_paddy:0.4f}\\\\niou:{val_iou:0.4f} | Paddy iou: {val_iou_paddy:0.4f} | Non-Paddy iou: {val_iou_non_paddy:0.4f}\")\\n', '            trainlog[epoch] = {\\n', '                \"train_loss\": train_loss,\\n', '                \"train_f1\": train_f1_macro.item(),\\n', '                \"train_f1_paddy\": train_f1_paddy.item(),\\n', '                \"train_f1_non_paddy\": train_f1_non_paddy.item(),\\n', '                \"train_acc\": train_acc.item(),\\n', '                \"train_acc_paddy\": train_acc_paddy.item(),\\n', '                \"train_acc_non_paddy\": train_acc_non_paddy.item(),\\n', '                \"train_iou\": train_iou.item(),\\n', '                \"train_iou_paddy\": train_iou_paddy.item(),\\n', '                \"train_iou_non_paddy\": train_iou_non_paddy.item(),\\n', '                \"val_loss\": val_loss,\\n', '                \"val_f1\": val_f1_macro.item(),\\n', '                \"val_f1_paddy\": val_f1_paddy.item(),\\n', '                \"val_f1_non_paddy\": val_f1_non_paddy.item(),\\n', '                \"val_acc\": val_acc.item(),\\n', '                \"val_acc_paddy\": val_acc_paddy.item(),\\n', '                \"val_acc_non_paddy\": val_acc_non_paddy.item(),\\n', '                \"val_iou\": val_iou.item(),\\n', '                \"val_iou_paddy\": val_iou_paddy.item(),\\n', '                \"val_iou_non_paddy\": val_iou_non_paddy.item(),\\n', '                \"lr\": lr\\n', '            }\\n', '        else:\\n', '            # train metrics\\n', '            train_rmse, train_mae, train_mape = train_metrics\\n', '            # val metrics\\n', '            val_rmse, val_mae, val_mape = val_metrics\\n', '            deciding_metric = val_mae\\n', '            print(f\"Val RMSE: {val_rmse:0.4f} | Val MAE: {val_mae:0.4f} | Val MAPE: {val_mape:0.4f}\")\\n', '            trainlog[epoch] = {\\n', '                \"train_loss\": train_loss,\\n', '                \"train_rmse\": train_rmse.item(),\\n', '                \"train_mae\": train_mae.item(),\\n', '                \"train_mape\": train_mape.item(),\\n', '                \"val_loss\": val_loss,\\n', '                \"val_rmse\": val_rmse.item(),\\n', '                \"val_mae\": val_mae.item(),\\n', '                \"val_mape\": val_mape.item(),\\n', '                \"lr\": lr,\\n', '            }\\n', '\\n', '        checkpoint(trainlog, CFG)\\n', '        if CFG.wandb:\\n', '            wandb.log(trainlog[epoch])\\n', '\\n', '        save_dict = {\\n', '            \"epoch\": epoch,\\n', '            \"optimizer\": optimizer.state_dict(),\\n', '            \"model\": model.state_dict()\\n', '        }\\n', '\\n', '        if (deciding_metric > best_metric and CFG.task == \"crop_type\") or (\\n', '                deciding_metric < best_metric and CFG.task != \"crop_type\"):\\n', '            print(f\"Valid Score Improved ({best_metric:0.4f} ---> {deciding_metric:0.4f})\")\\n', '            best_metric = deciding_metric\\n', '            torch.save(\\n', '                save_dict,\\n', '                os.path.join(\\n', '                    CFG.run_path, \"checkpoint_best.pth.tar\"\\n', '                ),\\n', '            )\\n', '        torch.save(\\n', '            save_dict,\\n', '            os.path.join(\\n', '                CFG.run_path, \"checkpoint_last.pth.tar\"\\n', '            ),\\n', '        )\\n', '\\n', '    print(\"Testing best epoch . . .\")\\n', '    best_checkpoint = torch.load(\\n', '        os.path.join(\\n', '            CFG.run_path, \"checkpoint_best.pth.tar\"\\n', '        )\\n', '    )\\n', '    model.load_state_dict(best_checkpoint[\"model\"])\\n', '    model.eval()\\n', '    arg_dict = dict(\\n', '        model=model,\\n', '        data_loader=val_loader,\\n', '        criterion=criterion,\\n', '        optimizer=optimizer,\\n', '        mode=\"val\",\\n', '        device=device,\\n', '        task=CFG.task,\\n', '    )\\n', '\\n', '    val_loss, val_metrics, wandb_table = iterate(log=True, **arg_dict)\\n', '    print(\"Validation Result\")\\n', '    if CFG.task == \"crop_type\":\\n', '        # test metric\\n', '        best_val_f1_macro, best_val_acc, best_val_iou, best_val_f1_paddy, best_val_f1_non_paddy, \\\\\\n', '        best_val_acc_paddy, best_val_acc_non_paddy, best_val_iou_paddy, best_val_iou_non_paddy, _ = val_metrics\\n', '        deciding_metric = best_val_f1_macro\\n', '        # log and print metrics\\n', '        print(\\n', '            f\"F1: {best_val_f1_macro:0.4f} | Paddy F1: {best_val_f1_paddy:0.4f} | Non-Paddy F1: {best_val_f1_non_paddy:0.4f} \\\\nAcc:{best_val_acc:0.4f} | Paddy Acc: {best_val_acc_paddy:0.4f} | Non-Paddy Acc: {best_val_acc_non_paddy:0.4f}\\\\niou:{best_val_iou:0.4f} | Paddy iou: {best_val_iou_paddy:0.4f} | Non-Paddy iou: {best_val_iou_non_paddy:0.4f}\")\\n', '\\n', '    else:\\n', '        # test metrics\\n', '        best_val_rmse, best_val_mae, best_val_mape = val_metrics\\n', '        print(f\"Test RMSE: {best_val_rmse:0.4f} | Test MAE: {best_val_mae:0.4f} | Test MAPE: {best_val_mape:0.4f}\")\\n', '\\n', '    if CFG.wandb:\\n', '        wandb.log({f\"{CFG.primary_sat}_val_prediction\": wandb_table})\\n', '\\n', '    test_loss, test_metrics, wandb_table = iterate(\\n', '        model,\\n', '        data_loader=test_loader,\\n', '        criterion=criterion,\\n', '        optimizer=optimizer,\\n', '        mode=\"test\",\\n', '        device=device,\\n', '        task=CFG.task,\\n', '        log=True\\n', '    )\\n', '    print(\"Test Result\")\\n', '    if CFG.task == \"crop_type\":\\n', '        # test metric\\n', '        test_f1_macro, test_acc, test_iou, test_f1_paddy, test_f1_non_paddy, \\\\\\n', '        test_acc_paddy, test_acc_non_paddy, test_iou_paddy, test_iou_non_paddy, _ = test_metrics\\n', '        deciding_metric = test_f1_macro\\n', '        # log and print metrics\\n', '        print(\\n', '            f\"F1: {test_f1_macro:0.4f} | Paddy F1: {test_f1_paddy:0.4f} | Non-Paddy F1: {test_f1_non_paddy:0.4f} \\\\nAcc:{test_acc:0.4f} | Paddy Acc: {test_acc_paddy:0.4f} | Non-Paddy Acc: {test_acc_non_paddy:0.4f}\\\\niou:{test_iou:0.4f} | Paddy iou: {test_iou_paddy:0.4f} | Non-Paddy iou: {test_iou_non_paddy:0.4f}\")\\n', '        log_wandb(test_loss, test_metrics, {\"key\": f\"{CFG.primary_sat}_test_prediction\", \"value\": wandb_table}, phase=\"test\")\\n', '\\n', '    else:\\n', '        # test metrics\\n', '        test_rmse, test_mae, test_mape = test_metrics\\n', '        print(f\"Test RMSE: {test_rmse:0.4f} | Test MAE: {test_mae:0.4f} | Test MAPE: {test_mape:0.4f}\")\\n', '        testlog = {\\n', '            \"test_loss\": test_loss,\\n', '            \"test_rmse\": test_rmse.item(),\\n', '            \"test_mae\": test_mae.item(),\\n', '            \"test_mape\": test_mape.item(),\\n', '            f\"{CFG.primary_sat}_test_prediction\": wandb_table,\\n', '            \"lr\": lr,\\n', '        }\\n', '        if CFG.wandb:\\n', '            wandb.log(testlog)\\n', '    # log model to wandb \\n', '    if CFG.wandb:\\n', \"        best = wandb.Artifact('checkpoint_best', type='model')\\n\", '        best.add_file(os.path.join(CFG.run_path, \"checkpoint_best.pth.tar\"))\\n', \"        last = wandb.Artifact('checkpoint_last', type='model')\\n\", '        last.add_file(os.path.join(CFG.run_path, \"checkpoint_last.pth.tar\"))\\n', '        wandb.log_artifact(best)\\n', '        wandb.log_artifact(last)\\n', '\\n', '\\n', 'if __name__ == \"__main__\":\\n', '    import warnings\\n', '\\n', '    warnings.filterwarnings(\"ignore\")\\n', '\\n', '    CFG = parser.parse_args()\\n', '    set_seed(CFG.seed)\\n', '    for k, v in vars(CFG).items():\\n', '        if k in list_args and v is not None:\\n', '            v = v.replace(\"[\", \"\")\\n', '            v = v.replace(\"]\", \"\")\\n', '            try:\\n', '                CFG.__setattr__(k, list(map(int, v.split(\",\"))))\\n', '            except:\\n', '                CFG.__setattr__(k, list(map(str, v.split(\",\"))))\\n', '                \\n', '    CFG.exp_name = CFG.task\\n', '    \\n', '    # if task type is regression. Increase lr and change output channel to 1 \\n', '    if CFG.task != \"crop_type\":\\n', '        # CFG.lr = 1e-1\\n', '        CFG.num_classes = 1\\n', '        # CFG.out_conv[-1] = 1\\n', '        \\n', '    # change out_conv incase of fusion\\n', '    # if len(CFG.satellites) >1:\\n', '    #     CFG.out_conv[-1] =  16\\n', '    # else:\\n', '    #     assert CFG.num_classes == CFG.out_conv[-1]\\n', '        \\n', '\\n', '    CFG.run_path = f\"runs/wacv_2024_seed{CFG.seed}/{CFG.exp_name}/{CFG.run_name}\"\\n', '    satellite_metadata = {\\n', '        \"S2\": {\\n', '            \"bands\": [\\'B1\\', \\'B2\\', \\'B3\\', \\'B4\\', \\'B5\\', \\'B6\\', \\'B7\\', \\'B8\\', \\'B8A\\', \\'B9\\', \\'B11\\', \\'B12\\'],\\n', '            \"rgb_bands\": [3, 2, 1],\\n', '            \"mask_res\": 10,\\n', '            \"img_size\": (32, 32),\\n', '        },\\n', '        \"S1\": {\\n', '            \"bands\": [\\'VV\\', \\'VH\\'],\\n', '            \"rgb_bands\": [0, 1, 0],\\n', '            \"mask_res\": 10,\\n', '            \"img_size\": (32, 32),\\n', '        },\\n', '        \"L8\": {\\n', '            \"bands\": [\"SR_B1\", \"SR_B2\", \"SR_B3\", \"SR_B4\", \"SR_B5\", \"SR_B6\", \"SR_B7\", \"ST_B10\"],\\n', '            \"rgb_bands\": [3, 2, 1],\\n', '            \"mask_res\": 10,\\n', '            \"img_size\": (32, 32),\\n', '        },\\n', '    }\\n', '    required_sat_data = {}\\n', '    for satellite in CFG.satellites:\\n', '        required_sat_data[satellite] = satellite_metadata[satellite]\\n', '    CFG.satellites = required_sat_data\\n', '    # first satellie is primary, img_size and mask_res is decided by it\\n', '    CFG.primary_sat =list(required_sat_data.keys())[0]\\n', '    CFG.img_size = required_sat_data[CFG.primary_sat][\"img_size\"]\\n', '\\n', '    # WandB\\n', '    if CFG.wandb:\\n', '        wandb.login()\\n', '        run = wandb.init(\\n', '            project=f\"wacv_2024_seed{CFG.seed}\",\\n', '            entity=\"agrifieldnet\",\\n', '            config={k: v for k, v in dict(vars(CFG)).items() if \"__\" not in k},\\n', '            name=CFG.run_name,\\n', '            group=CFG.exp_name,\\n', '        )\\n', '\\n', '    pprint.pprint(CFG)\\n', '    main(CFG)\\n']\n","output_type":"stream"}]},{"cell_type":"code","source":"for i, line in enumerate(content):\n    if \"--epochs\" in line:\n        content[i] = 'parser.add_argument(\"--epochs\", default=5, type=int, help=\"Number of epochs per fold\")\\n'\n        break","metadata":{"execution":{"iopub.status.busy":"2024-08-23T14:41:46.954554Z","iopub.execute_input":"2024-08-23T14:41:46.954896Z","iopub.status.idle":"2024-08-23T14:41:46.962748Z","shell.execute_reply.started":"2024-08-23T14:41:46.954871Z","shell.execute_reply":"2024-08-23T14:41:46.961866Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"with open('train.py', 'w') as file:\n    file.writelines(content)","metadata":{"execution":{"iopub.status.busy":"2024-08-23T14:41:46.963716Z","iopub.execute_input":"2024-08-23T14:41:46.963974Z","iopub.status.idle":"2024-08-23T14:41:46.973062Z","shell.execute_reply.started":"2024-08-23T14:41:46.963949Z","shell.execute_reply":"2024-08-23T14:41:46.972094Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"with open('train.py', 'r') as file:\n    content = file.readlines()\nprint(content)","metadata":{"execution":{"iopub.status.busy":"2024-08-23T14:41:46.974240Z","iopub.execute_input":"2024-08-23T14:41:46.974584Z","iopub.status.idle":"2024-08-23T14:41:46.983736Z","shell.execute_reply.started":"2024-08-23T14:41:46.974556Z","shell.execute_reply":"2024-08-23T14:41:46.982684Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"['#  conda install pytorch torchvision torchaudio pytorch-cuda=11.6 \\\\\\n', '    \\n', '# -c pytorch -c nvidia -c conda-forge\\n', '\"\"\"\\n', 'Main script for semantic experiments\\n', 'Built upon Vivien Sainte Fare Garnot (github/VSainteuf)\\n', 'License: MIT\\n', '\"\"\"\\n', '\\n', 'import argparse\\n', 'import json\\n', 'import os\\n', 'import copy\\n', 'import wandb\\n', 'import pprint\\n', '\\n', 'import time\\n', 'import random\\n', 'import pandas as pd\\n', 'import numpy as np\\n', 'from tqdm import tqdm\\n', '\\n', '# Custom import\\n', 'from utils.dataset import SICKLE_Dataset\\n', 'from utils import utae_utils, model_utils\\n', 'from utils.weight_init import weight_init\\n', 'from utils.metric import get_metrics, RMSELoss\\n', '# torch\\n', 'import torch\\n', 'import torch.nn as nn\\n', 'import torch.utils.data as data\\n', 'from torch.optim.lr_scheduler import CosineAnnealingLR\\n', 'import torchnet as tnt\\n', '\\n', 'parser = argparse.ArgumentParser()\\n', '# Model parameters\\n', 'parser.add_argument(\\n', '    \"--model\",\\n', '    default=\"utae\",\\n', '    type=str,\\n', '    help=\"Type of architecture to use. Can be one of: (utae/unet3d/fpn/convlstm/convgru/uconvlstm/buconvlstm)\",\\n', ')\\n', '## U-TAE Hyperparameters\\n', 'parser.add_argument(\"--encoder_widths\", default=\"[64,128]\", type=str)\\n', 'parser.add_argument(\"--decoder_widths\", default=\"[32,128]\", type=str)\\n', 'parser.add_argument(\"--out_conv\", default=\"[32, 16]\")\\n', 'parser.add_argument(\"--str_conv_k\", default=4, type=int)\\n', 'parser.add_argument(\"--str_conv_s\", default=2, type=int)\\n', 'parser.add_argument(\"--str_conv_p\", default=1, type=int)\\n', 'parser.add_argument(\"--agg_mode\", default=\"att_group\", type=str)\\n', 'parser.add_argument(\"--encoder_norm\", default=\"group\", type=str)\\n', 'parser.add_argument(\"--n_head\", default=16, type=int)\\n', 'parser.add_argument(\"--d_model\", default=256, type=int)\\n', 'parser.add_argument(\"--d_k\", default=4, type=int)\\n', '\\n', '# Set-up parameters\\n', 'parser.add_argument(\\n', '    \"--device\",\\n', '    default= \"cuda\" if torch.cuda.is_available() else \"cpu\",\\n', '    type=str,\\n', '    help=\"Name of device to use for tensor computations (cuda/cpu)\",\\n', ')\\n', 'parser.add_argument(\\n', '    \"--num_workers\", default=8, type=int, help=\"Number of data loading workers\"\\n', ')\\n', 'parser.add_argument(\"--seed\", default=0, type=int, help=\"Random seed\")\\n', '# Training parameters\\n', 'parser.add_argument(\"--epochs\", default=5, type=int, help=\"Number of epochs per fold\")\\n', 'parser.add_argument(\"--batch_size\", default=32, type=int, help=\"Batch size\")\\n', 'parser.add_argument(\"--lr\", default=1e-1, type=float, help=\"Learning rate\")\\n', '# parser.add_argument(\"--wd\", default=1e-2, type=float, help=\"weight decay\")\\n', 'parser.add_argument(\"--num_classes\", default=2, type=int)\\n', 'parser.add_argument(\"--ignore_index\", default=-999, type=int)\\n', 'parser.add_argument(\"--pad_value\", default=0, type=float)\\n', 'parser.add_argument(\"--padding_mode\", default=\"reflect\", type=str)\\n', 'parser.add_argument(\"--resume\", default=\"\", type=str, help=\"enter run path to resume\")\\n', 'parser.add_argument(\"--run_id\", default=\"\", type=str, help=\"enter run id to resume\")\\n', 'parser.add_argument(\"--wandb\", action=\\'store_true\\', help=\"debug?\")\\n', 'parser.add_argument(\\'--satellites\\', type=str, default=\"[S2]\")\\n', 'parser.add_argument(\\'--run_name\\', type=str, default=\"trial\")\\n', 'parser.add_argument(\\'--exp_name\\', type=str, default=\"utae\")\\n', 'parser.add_argument(\\'--task\\', type=str, default=\"crop_type\",\\n', '                    help=\"Available Tasks are crop_type, sowing_date, transplanting_date, harvesting_date, crop_yield\")\\n', 'parser.add_argument(\\'--actual_season\\', action=\\'store_true\\', help=\"whether to consider actual season or not.\")\\n', 'parser.add_argument(\\'--data_dir\\', type=str, default=\"../sickle_dev/data\")\\n', \"parser.add_argument('--use_augmentation', type=bool, default=True)\\n\", '\\n', 'list_args = [\"encoder_widths\", \"decoder_widths\", \"out_conv\", \"satellites\"]\\n', 'parser.set_defaults(cache=False)\\n', '\\n', '\\n', 'def recursive_todevice(x, device):\\n', '    if isinstance(x, torch.Tensor):\\n', '        return x.to(device)\\n', '    elif isinstance(x, dict):\\n', '        return {k: recursive_todevice(v, device) for k, v in x.items()}\\n', '    else:\\n', '        return [recursive_todevice(c, device) for c in x]\\n', '\\n', '\\n', 'def prepare_output(CFG):\\n', '    if CFG.wandb:\\n', '        if not os.path.exists(CFG.run_path):\\n', '            os.makedirs(CFG.run_path)\\n', '        elif CFG.resume:\\n', '            pass\\n', '        else:\\n', '            CFG.run_path = CFG.run_path + f\"_{time.time()}\"\\n', '            print(\"Run path already exist changed run path to \", CFG.run_path)\\n', '            os.makedirs(CFG.run_path)\\n', '    else:\\n', '        CFG.run_path += \"_debug\"\\n', '        os.makedirs(CFG.run_path, exist_ok=True)\\n', '\\n', '\\n', 'def checkpoint(log, config):\\n', '    with open(\\n', '            os.path.join(config.run_path, \"trainlog.json\"), \"w\"\\n', '    ) as outfile:\\n', '        json.dump(log, outfile, indent=4)\\n', '\\n', '\\n', 'def set_seed(seed=42):\\n', '    # Set a fixed value for the hash seed\\n', '    os.environ[\"PYTHONHASHSEED\"] = str(seed)\\n', '\\n', '    # For reproducibility\\n', '    np.random.seed(seed)\\n', '    random.seed(seed)\\n', '    torch.manual_seed(seed)\\n', '    torch.cuda.manual_seed(seed)\\n', '    torch.backends.cudnn.deterministic = True\\n', '    torch.backends.cudnn.benchmark = False\\n', '    try:\\n', '        torch.use_deterministic_algorithms(True, warn_only=True)\\n', '    except Exception as e:\\n', '        print(\"Can not use deterministic algorithm. Error: \", e)\\n', '    print(f\"> SEEDING DONE {seed}\")\\n', '\\n', '\\n', 'def log_wandb(loss, metrics, table=None, phase=\"train\"):\\n', '    f1_macro, acc, iou, f1_paddy, f1_non_paddy, \\\\\\n', '    acc_paddy, acc_non_paddy, iou_paddy, iou_non_paddy, (y_pred, y_true) = metrics\\n', '    y_pred, y_true = y_pred.tolist(), y_true.tolist()\\n', '    if CFG.wandb:\\n', '        log = {\\n', '                f\"{phase}_loss\": loss,\\n', '                f\"{phase}_f1_macro\": f1_macro,\\n', '                f\"{phase}_acc\": acc,\\n', '                f\"{phase}_iou\": iou,\\n', '                f\"{phase}_f1_paddy\": f1_paddy,\\n', '                f\"{phase}_f1_non_paddy\": f1_non_paddy,\\n', '                f\"{phase}_acc_paddy\": acc_paddy,\\n', '                f\"{phase}_acc_non_paddy\": acc_non_paddy,\\n', '                f\"{phase}_iou_paddy\": iou_paddy,\\n', '                f\"{phase}_iou_non_paddy\": iou_non_paddy,\\n', '            }\\n', '        if table is not None:\\n', '            log[table[\"key\"]] = table[\"value\"]\\n', '        wandb.log(log)\\n', '        if phase == \"test\":\\n', '            wandb.log({f\"{phase}_conf_mat\": wandb.plot.confusion_matrix(y_true=y_true, preds=y_pred, probs=None,\\n', '                                                                        class_names=[\"Paddy\", \"Non Paddy\"])})\\n', '\\n', '\\n', 'def iterate(\\n', '        model, data_loader, criterion, optimizer=None, scheduler=None, mode=\"train\", epoch=1, task=\"crop_type\",\\n', '        device=None, log=False, CFG=None,\\n', '):\\n', '    loss_meter = tnt.meter.AverageValueMeter()\\n', '    predictions = None\\n', '    targets = None\\n', '    pid_masks =None\\n', '    if log:\\n', '        columns = [\"image_l8\", \"image_s2\", \"image_s1\", \"gt_mask\", \"pred_filtered\", \"pred_whole\"]\\n', '        wandb_table = wandb.Table(columns=columns)\\n', '\\n', '    t_start = time.time()\\n', '    pbar = tqdm(enumerate(data_loader), total=len(data_loader), desc=mode)\\n', '    for i, batch in pbar:\\n', '        if device is not None:\\n', '            batch = recursive_todevice(batch, device)\\n', '        data, masks = batch\\n', '        plot_mask = masks[\"plot_mask\"]\\n', '        masks = masks[task]\\n', '        if task == \"crop_type\":\\n', '            masks = masks.long()\\n', '        else:\\n', '            masks = masks.float()\\n', '        if mode != \"train\":\\n', '            with torch.no_grad():\\n', '                y_pred = model(data)\\n', '        else:\\n', '            optimizer.zero_grad()\\n', '            y_pred = model(data)\\n', '        if task==\"crop_yield\": \\n', '            loss = criterion(y_pred, masks, plot_mask)\\n', '        else:\\n', '            loss = criterion(y_pred, masks)\\n', '            \\n', '        if mode == \"train\":\\n', '            loss.backward()\\n', '            optimizer.step()\\n', '\\n', '        # Compute Metric\\n', '        if task == \"crop_type\":\\n', '            y_pred = nn.Softmax(dim=1)(y_pred)\\n', '\\n', '        if predictions is None:\\n', '            predictions = y_pred\\n', '            targets = masks\\n', '            pid_masks = plot_mask\\n', '        else:\\n', '            predictions = torch.cat([predictions, y_pred], dim=0)\\n', '            targets = torch.cat([targets, masks], dim=0)\\n', '            pid_masks = torch.cat([pid_masks, plot_mask], dim=0)\\n', '            \\n', '\\n', '        if log:\\n', '            if len(data.keys()) == 3:\\n', '                (l8_images, l8_dates) = data[\"L8\"]\\n', '                (s2_images, s2_dates) = data[\"S2\"]\\n', '                (s1_images, s1_dates) = data[\"S1\"]\\n', '            else:\\n', '                (l8_images, l8_dates) = data[CFG.primary_sat]\\n', '                (s2_images, s2_dates) = data[CFG.primary_sat]\\n', '                (s1_images, s1_dates) = data[CFG.primary_sat]\\n', '            if task == \"crop_type\":\\n', '                # y_pred = torch.argmax(nn.Softmax(dim=1)(y_pred), dim=1)\\n', '                y_pred = nn.Softmax(dim=1)(y_pred)[:, 0, :, :]\\n', '                # log image of primary satellite\\n', '                l8_images, s2_images, s1_images, l8_dates, s2_dates, s1_dates, y_pred, masks = \\\\\\n', '                    l8_images.cpu().numpy(), s2_images.cpu().numpy(), s1_images.cpu().numpy(), \\\\\\n', '                    l8_dates.cpu().numpy(), s2_dates.cpu().numpy(), s1_dates.cpu().numpy(), \\\\\\n', '                    y_pred.cpu().numpy(), masks.cpu().numpy()\\n', '            else:\\n', '                # log image of primary satellite\\n', '                y_pred = y_pred[:, 0, :, :]\\n', '                l8_images, s2_images, s1_images, l8_dates, s2_dates, s1_dates, y_pred, masks = \\\\\\n', '                    l8_images.cpu().numpy(), s2_images.cpu().numpy(), s1_images.cpu().numpy(), \\\\\\n', '                    l8_dates.cpu().numpy(), s2_dates.cpu().numpy(), s1_dates.cpu().numpy(), \\\\\\n', '                    y_pred.cpu().numpy(), masks.cpu().numpy()\\n', '            log_test_predictions(l8_images, s2_images, s1_images, l8_dates, s2_dates, s1_dates, masks, y_pred, wandb_table, CFG=CFG)\\n', '\\n', '        loss_meter.add(loss.item())\\n', '\\n', '        # Just for Monitoring\\n', '        mem = torch.cuda.memory_reserved() / 1e9 if torch.cuda.is_available() else 0\\n', '        pbar.set_postfix(\\n', '            Loss=f\"{loss.item():0.4f}\",\\n', '            gpu_mem=f\"{mem:0.2f} GB\",\\n', '        )\\n', '    # take scheduler step\\n', '    if scheduler is not None and epoch < 3 * CFG.epochs // 4:\\n', '        scheduler.step()\\n', '\\n', '    t_end = time.time()\\n', '    total_time = t_end - t_start\\n', '    print(\"Epoch time : {:.1f}s\".format(total_time))\\n', '    metrics = get_metrics(predictions, targets, pid_masks, ignore_index=CFG.ignore_index, task=task)\\n', '    if log:\\n', '        return loss_meter.value()[0], metrics, wandb_table\\n', '    return loss_meter.value()[0], metrics\\n', '\\n', '\\n', 'n_log = 10  # no of samples to log\\n', '\\n', 'def generate_heatmap(mask):\\n', '    import matplotlib.pyplot as plt\\n', '    import seaborn as sns\\n', '    fig = plt.figure()\\n', '    hm = sns.heatmap(data=mask, vmin=-1, vmax=1 if np.max(mask) <= 1 else np.max(mask),\\n', \"                     cmap='RdYlGn')\\n\", \"    plt.axis('off')\\n\", '    fig.canvas.draw()\\n', '    mask = np.frombuffer(fig.canvas.tostring_rgb(), dtype=np.uint8)\\n', '    mask = mask.reshape(fig.canvas.get_width_height()[::-1] + (3,))\\n', '    return mask\\n', '\\n', '\\n', 'def log_test_predictions(l8_images, s2_images, s1_images, l8_dates, s2_dates, s1_dates, gt_masks, pred_masks, test_table, CFG = None, task=\"crop_type\", ):\\n', '    _id = 0\\n', '    # print(gt_masks.shape,pred_masks.shape)\\n', '    # pred_masks[pred_masks == 1] = 128\\n', '    gt_masks[gt_masks == -999] = -1\\n', '\\n', '\\n', '    # print(np.unique(pred_masks))\\n', '    for l8_sample, s2_sample, s1_sample, l8_sample_dates, s2_sample_dates, s1_sample_dates, gt_mask, pred_mask in \\\\\\n', '            zip(l8_images, s2_images, s1_images, l8_dates, s2_dates, s1_dates, gt_masks, pred_masks):\\n', '        # get last available image\\n', '        l8_image = l8_sample[len(l8_sample_dates[l8_sample_dates != 0]) - 1]\\n', '        # reshape and normalize image\\n', '        l8_image = l8_image[\\n', '            CFG.satellites[\"L8\" if len(CFG.satellites) == 3 else CFG.primary_sat][\"rgb_bands\"]].transpose(1, 2, 0)\\n', '        l8_image = ((l8_image - np.min(l8_image)) / (np.max(l8_image) - np.min(l8_image)))\\n', '\\n', '        s2_image = s2_sample[len(s2_sample_dates[s2_sample_dates != 0]) - 1]\\n', '        # reshape and normalize image\\n', '        s2_image = s2_image[\\n', '            CFG.satellites[\"S2\" if len(CFG.satellites) == 3 else CFG.primary_sat][\"rgb_bands\"]].transpose(1, 2, 0)\\n', '        s2_image = ((s2_image - np.min(s2_image)) / (np.max(s2_image) - np.min(s2_image)))\\n', '\\n', '        s1_image = s1_sample[len(s1_sample_dates[s1_sample_dates != 0]) - 1]\\n', '        # reshape and normalize image\\n', '        s1_image = s1_image[\\n', '            CFG.satellites[\"S1\" if len(CFG.satellites) == 3 else CFG.primary_sat][\"rgb_bands\"]].transpose(1, 2, 0)\\n', '        s1_image = ((s1_image - np.min(s1_image)) / (np.max(s1_image) - np.min(s1_image)))\\n', '\\n', '        # log whole prediction mask\\n', '        pred_mask_whole = generate_heatmap(copy.deepcopy(pred_mask))\\n', '        pred_mask[gt_mask == -1] = -1\\n', '        pred_mask = generate_heatmap(copy.deepcopy(pred_mask))\\n', '        if task == \"crop_type\":\\n', '            gt_mask[gt_mask == 0] = 2\\n', '            gt_mask[gt_mask == 1] = 0\\n', '            gt_mask[gt_mask == 2] = 1\\n', '        gt_mask = generate_heatmap(copy.deepcopy(gt_mask))\\n', '\\n', '        test_table.add_data(wandb.Image(l8_image), wandb.Image(s2_image), wandb.Image(s1_image), wandb.Image(gt_mask),\\n', '                            wandb.Image(pred_mask), wandb.Image(pred_mask_whole))\\n', '        _id += 1\\n', '        if _id == n_log:\\n', '            break\\n', '\\n', '\\n', 'def main(CFG):\\n', '    prepare_output(CFG)\\n', '    device = torch.device(CFG.device)\\n', '\\n', '    # Dataset definition\\n', '    data_dir = CFG.data_dir\\n', '    df = pd.read_csv(os.path.join(data_dir,\"sickle_dataset_tabular.csv\"))\\n', '    # if \"S2\" in CFG.satellites.keys():\\n', '    #     df = df[df[f\"S2_available\"] == True].reset_index(drop=True)\\n', '    # else:\\n', '    #     df = df[df[f\"{CFG.primary_sat}_available\"] == True].reset_index(drop=True)\\n', '    if CFG.task != \"crop_type\":\\n', '        df = df[df.YIELD > 0].reset_index(drop=True)\\n', '\\n', '    train_df = df[df.SPLIT == \"train\"].reset_index(drop=True)\\n', '    val_df = df[df.SPLIT == \"val\"].reset_index(drop=True)\\n', '    test_df = df[df.SPLIT == \"test\"].reset_index(drop=True)\\n', '\\n', '    dt_args = dict(\\n', '        data_dir=data_dir,\\n', '        satellites=CFG.satellites,\\n', '        ignore_index=CFG.ignore_index,\\n', '        transform=CFG.use_augmentation,\\n', '        actual_season=CFG.actual_season\\n', '    )\\n', '\\n', '    dt_train = SICKLE_Dataset(df=train_df, phase=\"train\", **dt_args)\\n', '    dt_args = dict(\\n', '        data_dir=data_dir,\\n', '        satellites=CFG.satellites,\\n', '        ignore_index=CFG.ignore_index,\\n', '        actual_season=CFG.actual_season\\n', '    )\\n', '    dt_val = SICKLE_Dataset(df=val_df, **dt_args, )\\n', '    dt_test = SICKLE_Dataset(df=test_df, **dt_args)\\n', '\\n', '    collate_fn = lambda x: utae_utils.pad_collate(x, pad_value=CFG.pad_value)\\n', '    train_loader = data.DataLoader(\\n', '        dt_train,\\n', '        batch_size=CFG.batch_size,\\n', '        shuffle=True,\\n', '        collate_fn=collate_fn,\\n', '        num_workers=CFG.num_workers,\\n', '    )\\n', '    val_loader = data.DataLoader(\\n', '        dt_val,\\n', '        batch_size=CFG.batch_size,\\n', '        shuffle=False,\\n', '        collate_fn=collate_fn,\\n', '        num_workers=CFG.num_workers,\\n', '    )\\n', '    test_loader = data.DataLoader(\\n', '        dt_test,\\n', '        batch_size=CFG.batch_size,\\n', '        shuffle=False,\\n', '        collate_fn=collate_fn,\\n', '        num_workers=CFG.num_workers,\\n', '    )\\n', '    batch_data, masks = next(iter(train_loader))\\n', '    for sat in CFG.satellites.keys():\\n', '        (samples, dates) = batch_data[sat]\\n', '        print(f\"-----------{sat}------------\")\\n', '        print(\"Samples Shape\", samples.shape, \"Masks Shape\", masks[\"crop_type\"].shape)\\n', '        print(\"dates\", dates[0])\\n', '        print(\"Samples\", torch.unique(samples[0]))\\n', '        print(\"Masks\", torch.unique(masks[CFG.task]))\\n', '\\n', '    print(\\n', '        \"Train {}, Val {}, Test {}\".format(len(dt_train), len(dt_val), len(dt_test))\\n', '    )\\n', '\\n', '    # Model definition\\n', '    # if len(CFG.satellites)==1:\\n', '    #     print(\"Using Build model\")\\n', '    #     model = model_utils.Build_model(CFG)\\n', '    # else:\\n', '    #     print(\"Using Fusion model\")\\n', '    #     model = model_utils.Fusion_model(CFG)\\n', '    model = model_utils.Fusion_model(CFG)\\n', '    model.apply(weight_init)\\n', '    model = model.to(device)\\n', '    CFG.N_params = utae_utils.get_ntrainparams(model)\\n', '    print(\"TOTAL TRAINABLE PARAMETERS :\", CFG.N_params)\\n', '    with open(os.path.join(CFG.run_path, \"conf.json\"), \"w\") as file:\\n', '        file.write(json.dumps(vars(CFG), indent=4))\\n', '\\n', '    # Optimizer, Loss and Scheduler\\n', '    optimizer = torch.optim.Adam(model.parameters(), lr=CFG.lr)\\n', '    if CFG.task == \"crop_type\":\\n', '        criterion = nn.CrossEntropyLoss(ignore_index=CFG.ignore_index,\\n', '                                        weight=torch.tensor([0.62013, 0.37987])).to(device=CFG.device, dtype=torch.float32)\\n', '    else:\\n', '        criterion = RMSELoss(ignore_index=CFG.ignore_index)\\n', '    scheduler = CosineAnnealingLR(optimizer, T_max=3 * CFG.epochs // 4, eta_min=1e-4)\\n', '\\n', '    # Training loop\\n', '    trainlog = {}\\n', '    best_metric = 0 if CFG.task == \"crop_type\" else torch.inf\\n', '    for epoch in range(1, CFG.epochs + 1):\\n', '        print(\"EPOCH {}/{}\".format(epoch, CFG.epochs))\\n', '        model.train()\\n', '        train_loss, train_metrics = iterate(\\n', '            model,\\n', '            data_loader=train_loader,\\n', '            criterion=criterion,\\n', '            optimizer=optimizer,\\n', '            scheduler=scheduler,\\n', '            mode=\"train\",\\n', '            device=device,\\n', '            epoch=epoch,\\n', '            task=CFG.task,\\n', '            CFG=CFG,\\n', '        )\\n', '\\n', '        print(\"Validation . . . \")\\n', '        model.eval()\\n', '        val_loss, val_metrics = iterate(\\n', '            model,\\n', '            data_loader=val_loader,\\n', '            criterion=criterion,\\n', '            optimizer=optimizer,\\n', '            mode=\"val\",\\n', '            device=device,\\n', '            task=CFG.task,\\n', '            CFG=CFG,\\n', '        )\\n', \"        lr = optimizer.param_groups[0]['lr']\\n\", '        if CFG.task == \"crop_type\":\\n', '            # train metrics\\n', '            train_f1_macro, train_acc, train_iou, train_f1_paddy, train_f1_non_paddy, \\\\\\n', '            train_acc_paddy, train_acc_non_paddy, train_iou_paddy, train_iou_non_paddy, _ = train_metrics\\n', '            # val metric\\n', '            val_f1_macro, val_acc, val_iou, val_f1_paddy, val_f1_non_paddy, \\\\\\n', '            val_acc_paddy, val_acc_non_paddy, val_iou_paddy, val_iou_non_paddy, _ = val_metrics\\n', '            deciding_metric = val_f1_macro\\n', '            # log and print metrics\\n', '            print(\\n', '                f\"F1: {val_f1_macro:0.4f} | Paddy F1: {val_f1_paddy:0.4f} | Non-Paddy F1: {val_f1_non_paddy:0.4f} \\\\nAcc:{val_acc:0.4f} | Paddy Acc: {val_acc_paddy:0.4f} | Non-Paddy Acc: {val_acc_non_paddy:0.4f}\\\\niou:{val_iou:0.4f} | Paddy iou: {val_iou_paddy:0.4f} | Non-Paddy iou: {val_iou_non_paddy:0.4f}\")\\n', '            trainlog[epoch] = {\\n', '                \"train_loss\": train_loss,\\n', '                \"train_f1\": train_f1_macro.item(),\\n', '                \"train_f1_paddy\": train_f1_paddy.item(),\\n', '                \"train_f1_non_paddy\": train_f1_non_paddy.item(),\\n', '                \"train_acc\": train_acc.item(),\\n', '                \"train_acc_paddy\": train_acc_paddy.item(),\\n', '                \"train_acc_non_paddy\": train_acc_non_paddy.item(),\\n', '                \"train_iou\": train_iou.item(),\\n', '                \"train_iou_paddy\": train_iou_paddy.item(),\\n', '                \"train_iou_non_paddy\": train_iou_non_paddy.item(),\\n', '                \"val_loss\": val_loss,\\n', '                \"val_f1\": val_f1_macro.item(),\\n', '                \"val_f1_paddy\": val_f1_paddy.item(),\\n', '                \"val_f1_non_paddy\": val_f1_non_paddy.item(),\\n', '                \"val_acc\": val_acc.item(),\\n', '                \"val_acc_paddy\": val_acc_paddy.item(),\\n', '                \"val_acc_non_paddy\": val_acc_non_paddy.item(),\\n', '                \"val_iou\": val_iou.item(),\\n', '                \"val_iou_paddy\": val_iou_paddy.item(),\\n', '                \"val_iou_non_paddy\": val_iou_non_paddy.item(),\\n', '                \"lr\": lr\\n', '            }\\n', '        else:\\n', '            # train metrics\\n', '            train_rmse, train_mae, train_mape = train_metrics\\n', '            # val metrics\\n', '            val_rmse, val_mae, val_mape = val_metrics\\n', '            deciding_metric = val_mae\\n', '            print(f\"Val RMSE: {val_rmse:0.4f} | Val MAE: {val_mae:0.4f} | Val MAPE: {val_mape:0.4f}\")\\n', '            trainlog[epoch] = {\\n', '                \"train_loss\": train_loss,\\n', '                \"train_rmse\": train_rmse.item(),\\n', '                \"train_mae\": train_mae.item(),\\n', '                \"train_mape\": train_mape.item(),\\n', '                \"val_loss\": val_loss,\\n', '                \"val_rmse\": val_rmse.item(),\\n', '                \"val_mae\": val_mae.item(),\\n', '                \"val_mape\": val_mape.item(),\\n', '                \"lr\": lr,\\n', '            }\\n', '\\n', '        checkpoint(trainlog, CFG)\\n', '        if CFG.wandb:\\n', '            wandb.log(trainlog[epoch])\\n', '\\n', '        save_dict = {\\n', '            \"epoch\": epoch,\\n', '            \"optimizer\": optimizer.state_dict(),\\n', '            \"model\": model.state_dict()\\n', '        }\\n', '\\n', '        if (deciding_metric > best_metric and CFG.task == \"crop_type\") or (\\n', '                deciding_metric < best_metric and CFG.task != \"crop_type\"):\\n', '            print(f\"Valid Score Improved ({best_metric:0.4f} ---> {deciding_metric:0.4f})\")\\n', '            best_metric = deciding_metric\\n', '            torch.save(\\n', '                save_dict,\\n', '                os.path.join(\\n', '                    CFG.run_path, \"checkpoint_best.pth.tar\"\\n', '                ),\\n', '            )\\n', '        torch.save(\\n', '            save_dict,\\n', '            os.path.join(\\n', '                CFG.run_path, \"checkpoint_last.pth.tar\"\\n', '            ),\\n', '        )\\n', '\\n', '    print(\"Testing best epoch . . .\")\\n', '    best_checkpoint = torch.load(\\n', '        os.path.join(\\n', '            CFG.run_path, \"checkpoint_best.pth.tar\"\\n', '        )\\n', '    )\\n', '    model.load_state_dict(best_checkpoint[\"model\"])\\n', '    model.eval()\\n', '    arg_dict = dict(\\n', '        model=model,\\n', '        data_loader=val_loader,\\n', '        criterion=criterion,\\n', '        optimizer=optimizer,\\n', '        mode=\"val\",\\n', '        device=device,\\n', '        task=CFG.task,\\n', '    )\\n', '\\n', '    val_loss, val_metrics, wandb_table = iterate(log=True, **arg_dict)\\n', '    print(\"Validation Result\")\\n', '    if CFG.task == \"crop_type\":\\n', '        # test metric\\n', '        best_val_f1_macro, best_val_acc, best_val_iou, best_val_f1_paddy, best_val_f1_non_paddy, \\\\\\n', '        best_val_acc_paddy, best_val_acc_non_paddy, best_val_iou_paddy, best_val_iou_non_paddy, _ = val_metrics\\n', '        deciding_metric = best_val_f1_macro\\n', '        # log and print metrics\\n', '        print(\\n', '            f\"F1: {best_val_f1_macro:0.4f} | Paddy F1: {best_val_f1_paddy:0.4f} | Non-Paddy F1: {best_val_f1_non_paddy:0.4f} \\\\nAcc:{best_val_acc:0.4f} | Paddy Acc: {best_val_acc_paddy:0.4f} | Non-Paddy Acc: {best_val_acc_non_paddy:0.4f}\\\\niou:{best_val_iou:0.4f} | Paddy iou: {best_val_iou_paddy:0.4f} | Non-Paddy iou: {best_val_iou_non_paddy:0.4f}\")\\n', '\\n', '    else:\\n', '        # test metrics\\n', '        best_val_rmse, best_val_mae, best_val_mape = val_metrics\\n', '        print(f\"Test RMSE: {best_val_rmse:0.4f} | Test MAE: {best_val_mae:0.4f} | Test MAPE: {best_val_mape:0.4f}\")\\n', '\\n', '    if CFG.wandb:\\n', '        wandb.log({f\"{CFG.primary_sat}_val_prediction\": wandb_table})\\n', '\\n', '    test_loss, test_metrics, wandb_table = iterate(\\n', '        model,\\n', '        data_loader=test_loader,\\n', '        criterion=criterion,\\n', '        optimizer=optimizer,\\n', '        mode=\"test\",\\n', '        device=device,\\n', '        task=CFG.task,\\n', '        log=True\\n', '    )\\n', '    print(\"Test Result\")\\n', '    if CFG.task == \"crop_type\":\\n', '        # test metric\\n', '        test_f1_macro, test_acc, test_iou, test_f1_paddy, test_f1_non_paddy, \\\\\\n', '        test_acc_paddy, test_acc_non_paddy, test_iou_paddy, test_iou_non_paddy, _ = test_metrics\\n', '        deciding_metric = test_f1_macro\\n', '        # log and print metrics\\n', '        print(\\n', '            f\"F1: {test_f1_macro:0.4f} | Paddy F1: {test_f1_paddy:0.4f} | Non-Paddy F1: {test_f1_non_paddy:0.4f} \\\\nAcc:{test_acc:0.4f} | Paddy Acc: {test_acc_paddy:0.4f} | Non-Paddy Acc: {test_acc_non_paddy:0.4f}\\\\niou:{test_iou:0.4f} | Paddy iou: {test_iou_paddy:0.4f} | Non-Paddy iou: {test_iou_non_paddy:0.4f}\")\\n', '        log_wandb(test_loss, test_metrics, {\"key\": f\"{CFG.primary_sat}_test_prediction\", \"value\": wandb_table}, phase=\"test\")\\n', '\\n', '    else:\\n', '        # test metrics\\n', '        test_rmse, test_mae, test_mape = test_metrics\\n', '        print(f\"Test RMSE: {test_rmse:0.4f} | Test MAE: {test_mae:0.4f} | Test MAPE: {test_mape:0.4f}\")\\n', '        testlog = {\\n', '            \"test_loss\": test_loss,\\n', '            \"test_rmse\": test_rmse.item(),\\n', '            \"test_mae\": test_mae.item(),\\n', '            \"test_mape\": test_mape.item(),\\n', '            f\"{CFG.primary_sat}_test_prediction\": wandb_table,\\n', '            \"lr\": lr,\\n', '        }\\n', '        if CFG.wandb:\\n', '            wandb.log(testlog)\\n', '    # log model to wandb \\n', '    if CFG.wandb:\\n', \"        best = wandb.Artifact('checkpoint_best', type='model')\\n\", '        best.add_file(os.path.join(CFG.run_path, \"checkpoint_best.pth.tar\"))\\n', \"        last = wandb.Artifact('checkpoint_last', type='model')\\n\", '        last.add_file(os.path.join(CFG.run_path, \"checkpoint_last.pth.tar\"))\\n', '        wandb.log_artifact(best)\\n', '        wandb.log_artifact(last)\\n', '\\n', '\\n', 'if __name__ == \"__main__\":\\n', '    import warnings\\n', '\\n', '    warnings.filterwarnings(\"ignore\")\\n', '\\n', '    CFG = parser.parse_args()\\n', '    set_seed(CFG.seed)\\n', '    for k, v in vars(CFG).items():\\n', '        if k in list_args and v is not None:\\n', '            v = v.replace(\"[\", \"\")\\n', '            v = v.replace(\"]\", \"\")\\n', '            try:\\n', '                CFG.__setattr__(k, list(map(int, v.split(\",\"))))\\n', '            except:\\n', '                CFG.__setattr__(k, list(map(str, v.split(\",\"))))\\n', '                \\n', '    CFG.exp_name = CFG.task\\n', '    \\n', '    # if task type is regression. Increase lr and change output channel to 1 \\n', '    if CFG.task != \"crop_type\":\\n', '        # CFG.lr = 1e-1\\n', '        CFG.num_classes = 1\\n', '        # CFG.out_conv[-1] = 1\\n', '        \\n', '    # change out_conv incase of fusion\\n', '    # if len(CFG.satellites) >1:\\n', '    #     CFG.out_conv[-1] =  16\\n', '    # else:\\n', '    #     assert CFG.num_classes == CFG.out_conv[-1]\\n', '        \\n', '\\n', '    CFG.run_path = f\"runs/wacv_2024_seed{CFG.seed}/{CFG.exp_name}/{CFG.run_name}\"\\n', '    satellite_metadata = {\\n', '        \"S2\": {\\n', '            \"bands\": [\\'B1\\', \\'B2\\', \\'B3\\', \\'B4\\', \\'B5\\', \\'B6\\', \\'B7\\', \\'B8\\', \\'B8A\\', \\'B9\\', \\'B11\\', \\'B12\\'],\\n', '            \"rgb_bands\": [3, 2, 1],\\n', '            \"mask_res\": 10,\\n', '            \"img_size\": (32, 32),\\n', '        },\\n', '        \"S1\": {\\n', '            \"bands\": [\\'VV\\', \\'VH\\'],\\n', '            \"rgb_bands\": [0, 1, 0],\\n', '            \"mask_res\": 10,\\n', '            \"img_size\": (32, 32),\\n', '        },\\n', '        \"L8\": {\\n', '            \"bands\": [\"SR_B1\", \"SR_B2\", \"SR_B3\", \"SR_B4\", \"SR_B5\", \"SR_B6\", \"SR_B7\", \"ST_B10\"],\\n', '            \"rgb_bands\": [3, 2, 1],\\n', '            \"mask_res\": 10,\\n', '            \"img_size\": (32, 32),\\n', '        },\\n', '    }\\n', '    required_sat_data = {}\\n', '    for satellite in CFG.satellites:\\n', '        required_sat_data[satellite] = satellite_metadata[satellite]\\n', '    CFG.satellites = required_sat_data\\n', '    # first satellie is primary, img_size and mask_res is decided by it\\n', '    CFG.primary_sat =list(required_sat_data.keys())[0]\\n', '    CFG.img_size = required_sat_data[CFG.primary_sat][\"img_size\"]\\n', '\\n', '    # WandB\\n', '    if CFG.wandb:\\n', '        wandb.login()\\n', '        run = wandb.init(\\n', '            project=f\"wacv_2024_seed{CFG.seed}\",\\n', '            entity=\"agrifieldnet\",\\n', '            config={k: v for k, v in dict(vars(CFG)).items() if \"__\" not in k},\\n', '            name=CFG.run_name,\\n', '            group=CFG.exp_name,\\n', '        )\\n', '\\n', '    pprint.pprint(CFG)\\n', '    main(CFG)\\n']\n","output_type":"stream"}]},{"cell_type":"code","source":"##Gadbad Code\nwith open('train.py', 'r') as file:\n    print(file.read())","metadata":{"execution":{"iopub.status.busy":"2024-08-23T14:41:46.984955Z","iopub.execute_input":"2024-08-23T14:41:46.985229Z","iopub.status.idle":"2024-08-23T14:41:46.993348Z","shell.execute_reply.started":"2024-08-23T14:41:46.985202Z","shell.execute_reply":"2024-08-23T14:41:46.991827Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stderr","text":"UsageError: Cell magic `%%Gadbad` not found.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Requirements","metadata":{"_uuid":"13097a03-4a9e-4df2-8fff-882dc1c6a06b","_cell_guid":"171e5d94-4ea8-4274-ac00-4be4c5cdcff4","id":"6G3MnxqYB-lE","trusted":true}},{"cell_type":"code","source":"%%capture\n!pip install -r requirements.txt","metadata":{"_uuid":"a9a7887a-109c-47f7-8f55-f3d06ec5557b","_cell_guid":"6c3aa40f-dd86-40ca-a0d1-78cae3cf655f","collapsed":false,"id":"zNN50BK_OYGI","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-08-23T14:43:45.879943Z","iopub.execute_input":"2024-08-23T14:43:45.880965Z","iopub.status.idle":"2024-08-23T14:46:28.768765Z","shell.execute_reply.started":"2024-08-23T14:43:45.880912Z","shell.execute_reply":"2024-08-23T14:46:28.767374Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"! chmod 777 ./*","metadata":{"_uuid":"e4749e31-a934-412d-a6a0-64a691be6c60","_cell_guid":"2ca4995f-baea-4220-8b29-66faf691a7a0","collapsed":false,"id":"ZyIUn2cbQA2I","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-08-23T14:46:37.664702Z","iopub.execute_input":"2024-08-23T14:46:37.665557Z","iopub.status.idle":"2024-08-23T14:46:37.784302Z","shell.execute_reply.started":"2024-08-23T14:46:37.665520Z","shell.execute_reply":"2024-08-23T14:46:37.783176Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"!./train.sh /kaggle/input/sickle-dataset/sickle_dataset/ [S1,S2] unet3d","metadata":{"_uuid":"1c17fb95-1232-46a2-ab83-a41398190d20","_cell_guid":"f59861df-03cf-4cbc-b58e-ce8b81a0099f","collapsed":false,"id":"8GL84NiqP8SL","outputId":"addeb202-122a-4add-efc9-bde7ebd96c0d","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-08-23T14:46:40.378464Z","iopub.execute_input":"2024-08-23T14:46:40.379335Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"> SEEDING DONE 0\nNamespace(model='unet3d', encoder_widths=[64, 128], decoder_widths=[32, 128], out_conv=[32, 16], str_conv_k=4, str_conv_s=2, str_conv_p=1, agg_mode='att_group', encoder_norm='group', n_head=16, d_model=256, d_k=4, device='cpu', num_workers=8, seed=0, epochs=5, batch_size=32, lr=0.1, num_classes=2, ignore_index=-999, pad_value=0, padding_mode='reflect', resume='', run_id='', wandb=False, satellites={'S1': {'bands': ['VV', 'VH'], 'rgb_bands': [0, 1, 0], 'mask_res': 10, 'img_size': (32, 32)}, 'S2': {'bands': ['B1', 'B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'B8', 'B8A', 'B9', 'B11', 'B12'], 'rgb_bands': [3, 2, 1], 'mask_res': 10, 'img_size': (32, 32)}}, run_name='[S1,S2]_unet3d', exp_name='crop_type', task='crop_type', actual_season=False, data_dir='/kaggle/input/sickle-dataset/sickle_dataset/', use_augmentation=True, cache=False, run_path='runs/wacv_2024_seed0/crop_type/[S1,S2]_unet3d', primary_sat='S1', img_size=(32, 32))\n-----------S1------------\nSamples Shape torch.Size([32, 23, 2, 32, 32]) Masks Shape torch.Size([32, 32, 32])\ndates tensor([  3,   9,  15,  21,  27,  33,  45,  57,  69,  81,  93, 105, 117, 129,\n        141, 153, 165, 177,   0,   0,   0,   0,   0])\nSamples tensor([-2.5294e+01, -2.5246e+01, -2.4161e+01,  ...,  0.0000e+00,\n         5.1325e-03,  5.9624e-01])\nMasks tensor([-999.,    0.,    1.], dtype=torch.float64)\n-----------S2------------\nSamples Shape torch.Size([32, 70, 12, 32, 32]) Masks Shape torch.Size([32, 32, 32])\ndates tensor([  2,   5,   7,  10,  12,  15,  17,  20,  22,  25,  27,  30,  32,  35,\n         37,  40,  42,  45,  47,  50,  52,  55,  57,  60,  62,  65,  67,  70,\n         72,  75,  77,  80,  82,  85,  87,  90,  92,  95,  97, 100, 102, 105,\n        107, 110, 112, 115, 117, 120, 122, 125, 127, 130, 132, 135, 137, 140,\n        142, 145, 147, 150, 152, 155, 157, 162, 165, 167, 172, 175, 177, 182])\nSamples tensor([0.0000e+00, 1.0000e+00, 3.0000e+00,  ..., 1.6125e+04, 1.6165e+04,\n        1.6166e+04])\nMasks tensor([-999.,    0.,    1.], dtype=torch.float64)\nTrain 1937, Val 227, Test 206\nTOTAL TRAINABLE PARAMETERS : 4644674\nEPOCH 1/5\ntrain:   0%|                                             | 0/61 [00:00<?, ?it/s]","output_type":"stream"}]},{"cell_type":"markdown","source":"# Main Code","metadata":{"_uuid":"70d65b77-211e-459c-8beb-de6a0afeba1d","_cell_guid":"1949c417-18db-40aa-a08e-5fecc01e7bc8","id":"aS1ZLfNCCDpz","trusted":true}},{"cell_type":"code","source":"import argparse\nimport json\nimport os\nimport copy\nimport pprint\n\nimport time\nimport random\nimport pandas as pd\nimport numpy as np\nfrom tqdm import tqdm\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Custom import\nfrom utils.dataset import SICKLE_Dataset\nfrom utils import utae_utils, model_utils\nfrom utils.weight_init import weight_init\nfrom utils.metric import get_metrics, RMSELoss\nfrom evaluate import iterate as val_iter\nfrom train import iterate as train_iter\n\n# torch\nimport torch\nimport torch.nn as nn\nimport torch.utils.data as data\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\nimport torchnet as tnt","metadata":{"_uuid":"26b0201c-ec71-4c4a-ba91-b5ff206db310","_cell_guid":"04a5b712-4fc6-4fd8-8404-40d38a3eace8","collapsed":false,"id":"AdybITehOFjt","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-08-23T14:41:46.998882Z","iopub.status.idle":"2024-08-23T14:41:46.999234Z","shell.execute_reply.started":"2024-08-23T14:41:46.999069Z","shell.execute_reply":"2024-08-23T14:41:46.999087Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def set_seed(seed=42):\n    # Set a fixed value for the hash seed\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n\n    # For reproducibility\n    np.random.seed(seed)\n    random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    try:\n        torch.use_deterministic_algorithms(True, warn_only=True)\n    except Exception as e:\n        print(\"Can not use deterministic algorithm. Error: \", e)\n    print(f\"> SEEDING DONE {seed}\")","metadata":{"_uuid":"5a884be3-cae5-4024-867b-09f4118286b8","_cell_guid":"d7cecc88-568e-4be0-a18f-fc022a3482f1","collapsed":false,"id":"4RRUNDISc6mY","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-08-23T14:41:47.000365Z","iopub.status.idle":"2024-08-23T14:41:47.000694Z","shell.execute_reply.started":"2024-08-23T14:41:47.000529Z","shell.execute_reply":"2024-08-23T14:41:47.000545Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_data_loaders():\n  # Dataset definition\n    data_dir = CFG.data_dir\n    df = pd.read_csv(os.path.join(data_dir,\"sickle_dataset_tabular.csv\"))\n\n    if CFG.task != \"crop_type\":\n        df = df[df.YIELD > 0].reset_index(drop=True)\n\n    train_df = df[df.SPLIT == \"train\"].reset_index(drop=True)\n    val_df = df[df.SPLIT == \"val\"].reset_index(drop=True)\n    test_df = df[df.SPLIT == \"test\"].reset_index(drop=True)\n\n    dt_args = dict(\n        data_dir=data_dir,\n        satellites=CFG.satellites,\n        ignore_index=CFG.ignore_index,\n        transform=CFG.use_augmentation,\n        actual_season=CFG.actual_season\n    )\n\n    dt_train = SICKLE_Dataset(df=train_df, phase=\"train\", **dt_args)\n    dt_args = dict(\n        data_dir=data_dir,\n        satellites=CFG.satellites,\n        ignore_index=CFG.ignore_index,\n        actual_season=CFG.actual_season\n    )\n    dt_val = SICKLE_Dataset(df=val_df, **dt_args, )\n    dt_test = SICKLE_Dataset(df=test_df, **dt_args)\n\n    collate_fn = lambda x: utae_utils.pad_collate(x, pad_value=CFG.pad_value)\n    train_loader = data.DataLoader(\n        dt_train,\n        batch_size=CFG.batch_size,\n        shuffle=True,\n        collate_fn=collate_fn,\n        num_workers=CFG.num_workers,\n    )\n    val_loader = data.DataLoader(\n        dt_val,\n        batch_size=CFG.batch_size,\n        shuffle=False,\n        collate_fn=collate_fn,\n        num_workers=CFG.num_workers,\n    )\n    test_loader = data.DataLoader(\n        dt_test,\n        batch_size=CFG.batch_size,\n        shuffle=False,\n        collate_fn=collate_fn,\n        num_workers=CFG.num_workers,\n    )\n    batch_data, masks = next(iter(train_loader))\n    for sat in CFG.satellites.keys():\n        (samples, dates) = batch_data[sat]\n        print(f\"-----------{sat}------------\")\n        print(\"Samples Shape\", samples.shape, \"Masks Shape\", masks[\"crop_type\"].shape)\n        print(\"dates\", dates[0])\n        print(\"Samples\", torch.unique(samples[0]))\n        print(\"Masks\", torch.unique(masks[CFG.task]))\n\n    print(\n        \"Train {}, Val {}, Test {}\".format(len(dt_train), len(dt_val), len(dt_test))\n    )\n    return train_loader, val_loader, test_loader","metadata":{"_uuid":"8830bd11-fb89-45f3-9d7f-55531fe85000","_cell_guid":"c18ff5c5-3a3f-4c1f-a125-791a24cfa398","collapsed":false,"id":"oT_7M1Ykq-GA","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-08-23T14:41:47.001921Z","iopub.status.idle":"2024-08-23T14:41:47.002267Z","shell.execute_reply.started":"2024-08-23T14:41:47.002087Z","shell.execute_reply":"2024-08-23T14:41:47.002103Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def val(CFG):\n    device = CFG.device\n    train_loader, val_loader, test_loader = get_data_loaders()\n\n    model = model_utils.Fusion_model(CFG)\n    model.apply(weight_init)\n    model = model.to(device)\n    CFG.N_params = utae_utils.get_ntrainparams(model)\n    print(\"TOTAL TRAINABLE PARAMETERS :\", CFG.N_params)\n\n    # Optimizer, Loss and Scheduler\n    optimizer = torch.optim.Adam(model.parameters(), lr=CFG.lr)\n    if CFG.task == \"crop_type\":\n        criterion = nn.CrossEntropyLoss(ignore_index=CFG.ignore_index,\n                                        weight=torch.tensor([0.62013, 0.37987])).to(device=CFG.device, dtype=torch.float32)\n    else:\n        criterion = RMSELoss(ignore_index=CFG.ignore_index)\n    scheduler = CosineAnnealingLR(optimizer, T_max=3 * CFG.epochs // 4, eta_min=1e-4)\n\n    best_checkpoint = torch.load(\n        os.path.join(\n            CFG.best_path, \"checkpoint_best.pth.tar\"\n        )\n    )\n    model.load_state_dict(best_checkpoint[\"model\"])\n\n    model.eval()\n    val_loss, val_metrics, _ = val_iter(\n        model,\n        data_loader=val_loader,\n        criterion=criterion,\n        optimizer=optimizer,\n        mode=\"val\",\n        device=device,\n        task=CFG.task,\n        log=True,\n        CFG=CFG,\n    )\n    print(f\"val Result {CFG.task}\")\n    if CFG.task == \"crop_type\":\n        # val metric\n        val_f1_macro, val_acc, val_iou, val_f1_paddy, val_f1_non_paddy, \\\n        val_acc_paddy, val_acc_non_paddy, val_iou_paddy, val_iou_non_paddy, _ = val_metrics\n        deciding_metric = val_f1_macro\n        # log and print metrics\n        print(\n            f\"F1: {val_f1_macro:0.4f} | Paddy F1: {val_f1_paddy:0.4f} | Non-Paddy F1: {val_f1_non_paddy:0.4f} \\nAcc:{val_acc:0.4f} | Paddy Acc: {val_acc_paddy:0.4f} | Non-Paddy Acc: {val_acc_non_paddy:0.4f}\\niou:{val_iou:0.4f} | Paddy iou: {val_iou_paddy:0.4f} | Non-Paddy iou: {val_iou_non_paddy:0.4f}\")\n\n    else:\n        # val metrics\n        val_rmse, val_mae, val_mape = val_metrics\n        print(f\"val RMSE: {val_rmse:0.4f} | val MAE: {val_mae:0.4f} | val MAPE: {val_mape:0.4f}\")\n        vallog = {\n            \"val_loss\": val_loss,\n            \"val_rmse\": val_rmse.item(),\n            \"val_mae\": val_mae.item(),\n            \"val_mape\": val_mape.item(),\n        }\n\ndef train(CFG):\n    device = CFG.device\n\n    train_loader, val_loader, test_loader = get_data_loaders()\n\n    model = model_utils.Fusion_model(CFG)\n    model.apply(weight_init)\n    model = model.to(device)\n    CFG.N_params = utae_utils.get_ntrainparams(model)\n    print(\"TOTAL TRAINABLE PARAMETERS :\", CFG.N_params)\n\n    # Optimizer, Loss and Scheduler\n    optimizer = torch.optim.Adam(model.parameters(), lr=CFG.lr)\n    if CFG.task == \"crop_type\":\n        criterion = nn.CrossEntropyLoss(ignore_index=CFG.ignore_index,\n                                        weight=torch.tensor([0.62013, 0.37987])).to(device=CFG.device, dtype=torch.float32)\n    else:\n        criterion = RMSELoss(ignore_index=CFG.ignore_index)\n    scheduler = CosineAnnealingLR(optimizer, T_max=3 * CFG.epochs // 4, eta_min=1e-4)\n\n    # Training loop\n    trainlog = {}\n    best_metric = 0 if CFG.task == \"crop_type\" else torch.inf\n    for epoch in range(1, CFG.epochs + 1):\n        print(\"EPOCH {}/{}\".format(epoch, CFG.epochs))\n        model.train()\n        train_loss, train_metrics = train_iter(\n            model,\n            data_loader=train_loader,\n            criterion=criterion,\n            optimizer=optimizer,\n            scheduler=scheduler,\n            mode=\"train\",\n            device=device,\n            epoch=epoch,\n            task=CFG.task,\n            CFG = CFG,\n        )\n\n        print(\"Validation . . . \")\n        model.eval()\n        val_loss, val_metrics = train_iter(\n            model,\n            data_loader=val_loader,\n            criterion=criterion,\n            optimizer=optimizer,\n            mode=\"val\",\n            device=device,\n            task=CFG.task,\n            CFG = CFG,\n\n        )\n        lr = optimizer.param_groups[0]['lr']\n        if CFG.task == \"crop_type\":\n            # train metrics\n            train_f1_macro, train_acc, train_iou, train_f1_paddy, train_f1_non_paddy, \\\n            train_acc_paddy, train_acc_non_paddy, train_iou_paddy, train_iou_non_paddy, _ = train_metrics\n            # val metric\n            val_f1_macro, val_acc, val_iou, val_f1_paddy, val_f1_non_paddy, \\\n            val_acc_paddy, val_acc_non_paddy, val_iou_paddy, val_iou_non_paddy, _ = val_metrics\n            deciding_metric = val_f1_macro\n            # log and print metrics\n            print(\n                f\"F1: {val_f1_macro:0.4f} | Paddy F1: {val_f1_paddy:0.4f} | Non-Paddy F1: {val_f1_non_paddy:0.4f} \\nAcc:{val_acc:0.4f} | Paddy Acc: {val_acc_paddy:0.4f} | Non-Paddy Acc: {val_acc_non_paddy:0.4f}\\niou:{val_iou:0.4f} | Paddy iou: {val_iou_paddy:0.4f} | Non-Paddy iou: {val_iou_non_paddy:0.4f}\")\n        else:\n            # train metrics\n            train_rmse, train_mae, train_mape = train_metrics\n            # val metrics\n            val_rmse, val_mae, val_mape = val_metrics\n            deciding_metric = val_mae\n            print(f\"Val RMSE: {val_rmse:0.4f} | Val MAE: {val_mae:0.4f} | Val MAPE: {val_mape:0.4f}\")\n\n        if (deciding_metric > best_metric and CFG.task == \"crop_type\") or (\n                deciding_metric < best_metric and CFG.task != \"crop_type\"):\n            print(f\"Valid Score Improved ({best_metric:0.4f} ---> {deciding_metric:0.4f})\")\n            best_metric = deciding_metric","metadata":{"_uuid":"1d4c17bf-5bee-43df-ae23-95525ce95978","_cell_guid":"e5acf7fe-0b35-4211-a243-bb82eee54fbe","collapsed":false,"id":"ECFlBKJmUIh7","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-08-23T14:41:47.003696Z","iopub.status.idle":"2024-08-23T14:41:47.004054Z","shell.execute_reply.started":"2024-08-23T14:41:47.003888Z","shell.execute_reply":"2024-08-23T14:41:47.003910Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CFG:\n    model='utae'\n    encoder_widths=[64, 128]\n    decoder_widths=[32, 128]\n    out_conv=[32, 16]\n    str_conv_k=4\n    str_conv_s=2\n    str_conv_p=1\n    agg_mode='att_group'\n    encoder_norm='group'\n    n_head=16\n    d_model=256\n    d_k=4\n    padding_mode='reflect'\n    # SICKLE Specific Parameters\n    device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n    num_workers=2\n    seed=0\n    epochs=10\n    batch_size=32\n    lr=0.1\n    num_classes=2\n    ignore_index=-999\n    pad_value=0\n    resume=''\n    run_id=''\n    debug=False\n    actual_season=False\n    use_augmentation=True\n    cache=False\n\n\nCFG.satellites=[\"S1\"]\nCFG.task='crop_type'\nCFG.run_name = f'{CFG.satellites}_{CFG.model}'\nCFG.data_dir='/content/SICKLE/sickle_toy_dataset'\nCFG.run_path='/content/SICKLE/runs/wacv_2024/crop_type/[S1]_utae'\nCFG.best_path='/content/SICKLE/runs/wacv_2024/crop_type/[S1]_utae'\nCFG.exp_name = CFG.task\n\nCFG.run_path = f\"runs/wacv_2024_seed{CFG.seed}/{CFG.exp_name}/{CFG.run_name}\"\n\nsatellite_metadata = {\n    \"S2\": {\n        \"bands\": ['B1', 'B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'B8', 'B8A', 'B9', 'B11', 'B12'],\n        \"rgb_bands\": [3, 2, 1],\n        \"mask_res\": 10,\n        \"img_size\": (32, 32),\n    },\n    \"S1\": {\n        \"bands\": ['VV', 'VH'],\n        \"rgb_bands\": [0, 1, 0],\n        \"mask_res\": 10,\n        \"img_size\": (32, 32),\n    },\n    \"L8\": {\n        \"bands\": [\"SR_B1\", \"SR_B2\", \"SR_B3\", \"SR_B4\", \"SR_B5\", \"SR_B6\", \"SR_B7\", \"ST_B10\"],\n        \"rgb_bands\": [3, 2, 1],\n        \"mask_res\": 10,\n        \"img_size\": (32, 32),\n    },\n}\n\nrequired_sat_data = {}\nfor satellite in CFG.satellites:\n    required_sat_data[satellite] = satellite_metadata[satellite]\nCFG.satellites = required_sat_data\n\n# first satellie is primary, img_size and mask_res is decided by it\nCFG.primary_sat =list(required_sat_data.keys())[0]\nCFG.img_size = required_sat_data[CFG.primary_sat][\"img_size\"]\nset_seed(CFG.seed)","metadata":{"_uuid":"1c8a3398-42cd-470b-8c55-ae895899b0ad","_cell_guid":"3c8b5f3f-32f7-47d3-990b-5a978d0ca387","collapsed":false,"id":"ConYCm6eNoOt","outputId":"e55e12cc-18e0-44ee-a736-6326086392fa","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-08-23T14:41:47.005602Z","iopub.status.idle":"2024-08-23T14:41:47.005958Z","shell.execute_reply.started":"2024-08-23T14:41:47.005762Z","shell.execute_reply":"2024-08-23T14:41:47.005778Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Validation and Training","metadata":{"_uuid":"1e2d4164-fa6f-4937-b049-16edf93a2e87","_cell_guid":"1b02d03a-3cbd-4e7b-8165-1bf86155bf7b","id":"mhWHvUiaBptZ","trusted":true}},{"cell_type":"code","source":"val(CFG)","metadata":{"_uuid":"4bc154ef-e1a5-4df2-b759-73485681ca75","_cell_guid":"adaebd82-a886-4023-bcd3-1ec6012b3c6d","collapsed":false,"id":"nU-p2YZ5TBUo","outputId":"4fb8d660-dccc-4a4f-ba02-b2c99597f559","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-08-23T14:41:47.007239Z","iopub.status.idle":"2024-08-23T14:41:47.007605Z","shell.execute_reply.started":"2024-08-23T14:41:47.007425Z","shell.execute_reply":"2024-08-23T14:41:47.007443Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train(CFG)","metadata":{"_uuid":"dc156d9b-77d6-4f79-91a7-398b39ed54f7","_cell_guid":"9f5555f2-b3e4-45eb-8854-f5b8c786a1c6","collapsed":false,"id":"cFkh3EKZpuit","outputId":"72150a65-6850-4e67-de01-c7372d59ef3b","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-08-23T14:41:47.008488Z","iopub.status.idle":"2024-08-23T14:41:47.008810Z","shell.execute_reply.started":"2024-08-23T14:41:47.008654Z","shell.execute_reply":"2024-08-23T14:41:47.008670Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"_uuid":"25051e20-ff31-4d6d-953f-e77c6a870fd2","_cell_guid":"6a9f0133-1f45-41d1-a491-107d919d7a0e","collapsed":false,"id":"m19rwJncuZ86","jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]}]}